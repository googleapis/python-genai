import google.generativeai as genai
import collections
import random
import math
from google.generativeai.types import GenerationConfig, GenerateContentResponse


def _normalize_probs(prob_map):
    """Normalizes a dictionary of token probabilities."""
    total_prob = sum(prob_map.values())
    if total_prob == 0:
        num_tokens = len(prob_map)
        if num_tokens == 0:
            return {}
        # Assign equal probability if total is zero (should be rare)
        equal_prob = 1.0 / num_tokens
        return {token: equal_prob for token in prob_map}

    return {token: prob / total_prob for token, prob in prob_map.items()}


def generate_content_min_p_approx(
    model: genai.GenerativeModel,
    prompt: str,
    max_new_tokens: int = 50,
    p_base: float = 0.1,
    temperature: float = 1.0,
    num_samples: int = 100,
    fallback_to_most_frequent: bool = True
) -> str:
    """
    Generates content using a client-side approximation of min_p sampling.

    This function simulates min_p sampling by generating multiple candidates
    at each step, estimating probabilities, filtering based on min_p logic,
    and then sampling from the filtered set.

    Note: This is significantly slower and more resource-intensive than native
          API-level sampling methods.

    Args:
        model: An initialized GenerativeModel instance.
        prompt: The initial text prompt.
        max_new_tokens: The maximum number of tokens to generate.
        p_base: The base factor for min_p calculation (threshold = p_base * p_max).
                Must be between 0.0 (exclusive) and 1.0 (inclusive).
        temperature: The temperature to use when generating candidate tokens.
        num_samples: The number of candidates to generate at each step to
                     estimate probabilities. Higher values give better estimates
                     but increase latency and cost.
        fallback_to_most_frequent: If True, falls back to the most frequent
                                   token if the min_p filter removes all tokens.
                                   If False, stops generation in that case.

    Returns:
        The generated text sequence including the initial prompt.

    Raises:
        ValueError: If p_base is not within the valid range (0.0, 1.0].
        Exception: Catches and prints exceptions during the generation process.
    """
    if not (0.0 < p_base <= 1.0):
        raise ValueError("p_base must be between 0.0 (exclusive) and 1.0 (inclusive)")

    generated_sequence = prompt
    current_prompt = prompt

    print(f"Starting min_p approximation (p_base={p_base}, temp={temperature}, samples={num_samples})...")

    for i in range(max_new_tokens):
        print(f"  Step {i+1}/{max_new_tokens}: Generating candidates...")
        try:
            # Configure the model to generate multiple short candidates
            estimation_config = GenerationConfig(
                candidate_count=num_samples,
                max_output_tokens=1, # We only need the next token
                temperature=temperature
                # Note: We cannot use top_p/top_k here as we need a wider sample
                #       to estimate the distribution for min_p.
            )

            # Generate candidates for the next token
            response: GenerateContentResponse = model.generate_content(
                current_prompt,
                generation_config=estimation_config,
                # stream=False # Ensure we get all candidates at once
            )

            if not response.candidates:
                 print("  Warning: No candidates generated by the API. Stopping generation.")
                 break # Stop if no candidates are returned

            # --- Estimate probabilities from candidate frequencies ---
            token_counts = collections.Counter()
            valid_candidates = 0
            for candidate in response.candidates:
                # Check for valid content and safety
                if candidate.content and candidate.content.parts and candidate.finish_reason == 1: # FINISH_REASON_STOP
                    try:
                         token_text = candidate.content.parts[0].text
                         if token_text: # Ensure token is not empty
                            token_counts[token_text] += 1
                            valid_candidates += 1
                    except (IndexError, AttributeError, KeyError):
                        # Log unexpected structure but continue
                        print(f"  Warning: Skipping malformed candidate: {candidate}")
                        continue
                else:
                    # Log reasons for skipping (e.g., safety, recitation)
                     reason = candidate.finish_reason
                     ratings = candidate.safety_ratings
                     print(f"  Warning: Skipping candidate (Finish Reason: {reason}, Safety: {ratings})")


            if valid_candidates == 0:
                print("  Error: No valid tokens found among candidates after filtering. Stopping generation.")
                break # Stop if no usable tokens were generated

            # Calculate estimated probabilities
            estimated_probs = {token: count / valid_candidates for token, count in token_counts.items()}

            if not estimated_probs:
                 print("  Error: Estimated probabilities are empty. Stopping generation.")
                 break # Should not happen if valid_candidates > 0, but safety check

            # --- Apply Min-P filtering ---
            p_max = max(estimated_probs.values())
            p_threshold = p_base * p_max

            filtered_probs = {
                token: prob for token, prob in estimated_probs.items() if prob >= p_threshold
            }

            # --- Select the next token ---
            chosen_token = None
            if not filtered_probs:
                print(f"  Warning: Min_p filter (p_base={p_base}, p_max={p_max:.4f}, threshold={p_threshold:.4f}) resulted in empty set.")
                if fallback_to_most_frequent and token_counts:
                    # Fallback to the most frequent token if allowed
                    chosen_token = token_counts.most_common(1)[0][0]
                    print(f"    Falling back to most frequent token: '{chosen_token}' (count: {token_counts[chosen_token]})")
                else:
                    print("  Error: No tokens passed min_p filter and no fallback. Stopping.")
                    break # Stop if filter is empty and no fallback
            else:
                # Normalize the filtered probabilities and sample
                normalized_filtered_probs = _normalize_probs(filtered_probs)
                tokens = list(normalized_filtered_probs.keys())
                probabilities = list(normalized_filtered_probs.values())

                # Choose one token based on the normalized filtered probabilities
                chosen_token = random.choices(tokens, weights=probabilities, k=1)[0]
                # print(f"    Chose token: '{chosen_token}' from {len(tokens)} options.")


            if chosen_token is None:
                 print("  Error: Could not select a token. Stopping generation.")
                 break # Safety break if token selection failed

            # Append the chosen token and update the prompt for the next step
            generated_sequence += chosen_token
            current_prompt += chosen_token # Use the growing sequence as the next prompt

        except Exception as e:
            print(f"  Error during generation step {i+1}: {e}")
            # Consider adding more specific error handling (e.g., for API errors)
            break # Stop on any unexpected error

    print("Finished min_p approximation.")
    return generated_sequence

# --- Example Usage ---
if __name__ == "__main__":
    print("Running min_p approximation example...")
    try:
        # --- Configuration ---
        # IMPORTANT: Configure your API key beforehand, e.g., via environment variable
        # or genai.configure(api_key="YOUR_API_KEY")
        # genai.configure(api_key="...") # Uncomment and add your key if needed

        model_name = 'gemini-1.5-flash-latest' # Or choose another compatible model
        model = genai.GenerativeModel(model_name)

        initial_prompt = "Write a short story opening about a curious robot exploring a vibrant alien jungle."

        # Min-P Approximation Parameters
        p_base_val = 0.08       # Lower values increase filtering (more like greedy), higher values decrease it
        temp_val = 1.2          # Higher temperature for more diverse candidates
        samples_per_step = 128  # Number of candidates per step (higher = better estimate, slower)
        tokens_to_generate = 150 # Desired length of the generated text (excluding prompt)

        print(f"\nModel: {model_name}")
        print(f'Initial Prompt: "{initial_prompt}"') # Use single quotes for f-string
        print(f"Parameters: p_base={p_base_val}, temperature={temp_val}, samples/step={samples_per_step}, max_tokens={tokens_to_generate}\n")

        # --- Run Generation ---
        generated_text = generate_content_min_p_approx(
            model,
            initial_prompt,
            max_new_tokens=tokens_to_generate,
            p_base=p_base_val,
            temperature=temp_val,
            num_samples=samples_per_step,
            fallback_to_most_frequent=True # Allow fallback if filter is too aggressive
        )

        # --- Output ---
        print("\n--- Generated Text ---")
        print(generated_text)
        print("----------------------")

    except Exception as e:
        print(f"\nAn error occurred during example usage: {e}")
        print("Please ensure you have configured your API key (e.g., GOOGLE_API_KEY environment variable)")
        print(f"and have access to the model '{model_name}'.") 